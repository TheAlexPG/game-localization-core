import os
import re
import time
import json
import threading
from pathlib import Path
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# ===== –ù–ê–õ–ê–®–¢–£–í–ê–ù–ù–Ø =====
SOURCE_DIR = "./SILKSONG_RU/._Decrypted"  # <<< –ó–ú–Ü–ù–ï–ù–û: –ü–∞–ø–∫–∞ –∑ —Ä–æ—Å—ñ–π—Å—å–∫–∏–º–∏ —Ñ–∞–π–ª–∞–º–∏
OUTPUT_DIR = "../SILKSONG_UA"  # –ü–∞–ø–∫–∞ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥–µ–Ω–∏—Ö —Ñ–∞–π–ª—ñ–≤
BATCH_SIZE = 10  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ –¥–ª—è –ø–µ—Ä–µ–∫–ª–∞–¥—É –∑–∞ —Ä–∞–∑
MAX_PARALLEL_FILES = 5  # –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ñ–∞–π–ª—ñ–≤ —â–æ –æ–±—Ä–æ–±–ª—è—é—Ç—å—Å—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ

# OpenAI –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
MODEL_NAME = "gpt-5"  # <<< –ó–ú–Ü–ù–ï–ù–û: –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–∫—Ç—É–∞–ª—å–Ω—É —Ç–∞ –ø–æ—Ç—É–∂–Ω—É –º–æ–¥–µ–ª—å
TEMPERATURE = 1  # <<< –ó–ú–Ü–ù–ï–ù–û: –ó–Ω–∏–∂–µ–Ω–æ –¥–ª—è –±—ñ–ª—å—à–æ—ó —Ç–æ—á–Ω–æ—Å—Ç—ñ —Ç–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—ñ
MAX_RETRIES = 3
RETRY_DELAY = 5  # –ó–±—ñ–ª—å—à–µ–Ω–æ –¥–ª—è –ø—É–±–ª—ñ—á–Ω–æ–≥–æ API

# –ì–ª–æ—Å–∞—Ä—ñ–π —Ç–µ—Ä–º—ñ–Ω—ñ–≤ —â–æ –Ω–µ –ø–µ—Ä–µ–∫–ª–∞–¥–∞—î–º–æ
PRESERVE_TERMS = [
    "Hornet", "Pharloom", "Silksong", "Citadel",
    "Garmond", "Lace", "Shakra", "Coral"
]

# –ì–ª–æ—Å–∞—Ä—ñ–π –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–∫–ª–∞–¥—É
GLOSSARY = {
    "Weaver": "–¢–∫–∞—á", "Weavers": "–¢–∫–∞—á—ñ",
    "Needle": "–ì–æ–ª–∫–∞",
    "Thread": "–ù–∏—Ç–∫–∞",
    "Silk": "–®–æ–≤–∫",
    "Hunter": "–ú–∏—Å–ª–∏–≤–µ—Ü—å",
    "Knight": "–õ–∏—Ü–∞—Ä",
    "Crest": "–ì–µ—Ä–±",
    "Wilds": "–ü—É—Å—Ç–∫–∞",
    "Forge": "–ö—É–∑–Ω—è",
    "Peak": "–ü—ñ–∫",
    "Void": "–ü–æ—Ä–æ–∂–Ω–µ—á–∞",
    "Soul": "–î—É—à–∞",
    "Mask": "–ú–∞—Å–∫–∞",
    "Charm": "–û–±–µ—Ä—ñ–≥",
    "Spool": "–ö–æ—Ç—É—à–∫–∞",
    "Fragment": "–§—Ä–∞–≥–º–µ–Ω—Ç"
}


class SilksongOpenAITranslator:
    def __init__(self):
        self.client = OpenAI(api_key=OPENAI_API_KEY)
        self.translation_cache = {}
        self.stats = {"translated": 0, "cached": 0, "errors": 0, "tokens_used": 0, "api_calls": 0}
        self.cache_lock = threading.Lock()
        self.stats_lock = threading.Lock()
        self.load_cache()

    def update_stats(self, **kwargs):
        with self.stats_lock:
            for key, value in kwargs.items():
                if key in self.stats:
                    self.stats[key] += value

    def get_from_cache(self, text: str):
        with self.cache_lock:
            return self.translation_cache.get(text)

    def add_to_cache(self, text: str, translation: str):
        with self.cache_lock:
            self.translation_cache[text] = translation

    def create_batch_prompt(self, texts: List[str]) -> str:
        # <<< –ó–ú–Ü–ù–ï–ù–û: –ü—Ä–æ–º–ø—Ç –∑–Ω–∞—á–Ω–æ –ø–æ–∫—Ä–∞—â–µ–Ω–æ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç—ñ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ç–µ–≥—ñ–≤
        glossary_str = "\n".join([f"- '{en}': '{ua}'" for en, ua in GLOSSARY.items()])
        preserve_str = ", ".join(PRESERVE_TERMS)

        prompt = f"""You are a meticulous game localizer translating "Hollow Knight: Silksong" from Russian to Ukrainian. Your task is to translate a batch of texts and return a single JSON object.

### CRITICAL INSTRUCTIONS ###
1.  **Translate from Russian to Ukrainian**: The source language is Russian.
2.  **Strict Tag Preservation**: The text contains special tags like `<page>`, `<hpage>`, `&lt;...&gt;`, `{{...}}`, `\\n`. You MUST copy these tags to the output **EXACTLY** as they appear in the original, without any changes.
3.  **Adhere to Glossary**: Strictly use the provided glossary for consistency.
    {glossary_str}
4.  **Do Not Translate Names**: Keep these names in English: {preserve_str}.
5.  **Maintain Style**: Preserve the dark, poetic, and atmospheric style of the original.
6.  **JSON Output**: Return ONLY a valid JSON object mapping the original index to the Ukrainian translation. Do not add any explanations or markdown.

### EXAMPLE ###
- **Input Texts**:
1. "–¢–∞–∫-—Ç–∞–∫! –ß—Ç–æ —ç—Ç–æ —É –Ω–∞—Å —Ç—É—Ç?&lt;page&gt;–ó–∞—Ä–∞–∂–µ–Ω–∏–µ?"
2. "–ò—Å–ø–æ–ª—å–∑—É–π —Å–≤–æ–π Crest"
- **Correct JSON Output**:
{{"1": "–¢–∞–∫-—Ç–∞–∫! –©–æ —Ü–µ –≤ –Ω–∞—Å —Ç—É—Ç?&lt;page&gt;–ó–∞—Ä–∞–∂–µ–Ω–Ω—è?", "2": "–í–∏–∫–æ—Ä–∏—Å—Ç–∞–π —Å–≤—ñ–π –ì–µ—Ä–±"}}

### TEXTS TO TRANSLATE (RUSSIAN) ###
"""
        for i, text in enumerate(texts, 1):
            prompt += f'{i}. "{text}"\n'

        return prompt

    def translate_batch(self, texts: List[str], file_name: str = "") -> List[str]:
        if not texts:
            return []

        texts_to_translate = []
        results = [None] * len(texts)
        indices_to_translate = []

        for i, text in enumerate(texts):
            if not text or text.startswith('$') or not re.search(r'[a-zA-Z–∞-—è–ê-–Ø]', text):
                results[i] = text
            else:
                cached = self.get_from_cache(text)
                if cached:
                    results[i] = cached
                    self.update_stats(cached=1)
                else:
                    texts_to_translate.append(text)
                    indices_to_translate.append(i)

        if not texts_to_translate:
            return [res for res in results if res is not None]

        for attempt in range(MAX_RETRIES):
            try:
                thread_id = threading.current_thread().name
                print(f"    [{thread_id}] {file_name}: Translating batch of {len(texts_to_translate)} texts...")

                response = self.client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=[
                        {
                            "role": "system",
                            "content": "You are a highly precise translation engine. You will receive a list of Russian texts and must return a single, valid JSON object with their Ukrainian translations, preserving all technical tags perfectly."
                        },
                        {
                            "role": "user",
                            "content": self.create_batch_prompt(texts_to_translate)
                        }
                    ],
                    temperature=TEMPERATURE,
                    max_completion_tokens=16000,  # –ú–∞–∫—Å–∏–º—É–º –¥–ª—è –±–∞–≥–∞—Ç—å–æ—Ö –º–æ–¥–µ–ª–µ–π
                    response_format={"type": "json_object"}
                )

                self.update_stats(api_calls=1, tokens_used=response.usage.total_tokens if response.usage else 0)

                translations_dict = json.loads(response.choices[0].message.content.strip())

                for i, (original_idx, text) in enumerate(zip(indices_to_translate, texts_to_translate), 1):
                    translation = translations_dict.get(str(i), f"[ERROR] {text}")
                    results[original_idx] = translation
                    self.add_to_cache(text, translation)
                    self.update_stats(translated=1)

                return [res for res in results if res is not None]

            except Exception as e:
                print(f"    [{thread_id}] API error (attempt {attempt + 1}/{MAX_RETRIES}): {e}")
                if "rate_limit" in str(e).lower():
                    time.sleep(RETRY_DELAY * (attempt + 1))
                elif attempt < MAX_RETRIES - 1:
                    time.sleep(RETRY_DELAY)

        for original_idx, text in zip(indices_to_translate, texts_to_translate):
            results[original_idx] = f"[ERROR] {text}"
            self.update_stats(errors=1)

        return [res for res in results if res is not None]

    def extract_entries(self, file_path: str) -> List[Tuple[str, str]]:
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            pattern = r'<entry name="([^"]+)">(.*?)</entry>'
            return re.findall(pattern, content, re.DOTALL)
        except Exception as e:
            print(f"Error reading {file_path}: {e}")
            return []

    def process_file(self, input_path: str, output_path: str, file_index: int, total_files: int):
        file_name = Path(input_path).name
        thread_id = threading.current_thread().name
        print(f"\n[{file_index}/{total_files}] üìÑ {thread_id} Processing: {file_name}")

        with open(input_path, 'r', encoding='utf-8') as f:
            content = f.read()

        entries = self.extract_entries(input_path)
        if not entries:
            print(f"  [{thread_id}] No entries found in {file_name}, copying.")
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return True

        print(f"  [{thread_id}] Found {len(entries)} entries in {file_name}")

        original_texts = [text for name, text in entries]
        translated_texts = []
        total_batches = (len(original_texts) + BATCH_SIZE - 1) // BATCH_SIZE

        for i in range(0, len(original_texts), BATCH_SIZE):
            batch_texts = original_texts[i:i + BATCH_SIZE]
            current_batch_num = (i // BATCH_SIZE) + 1
            print(f"  [{thread_id}] {file_name}: Processing batch {current_batch_num}/{total_batches}")
            translated_batch = self.translate_batch(batch_texts, file_name)
            translated_texts.extend(translated_batch)
            self.save_cache()  # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –∫–µ—à –ø—ñ—Å–ª—è –∫–æ–∂–Ω–æ–≥–æ –±–∞—Ç—á—É

        translated_content = content
        for (name, original), translated in zip(entries, translated_texts):
            # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —Ñ—É–Ω–∫—Ü—ñ—é –¥–ª—è –∑–∞–º—ñ–Ω–∏, —â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –ø–æ–º–∏–ª–æ–∫ –∑ –æ–¥–Ω–∞–∫–æ–≤–∏–º–∏ —Ä—è–¥–∫–∞–º–∏
            old_entry = f'<entry name="{name}">{original}</entry>'
            new_entry = f'<entry name="{name}">{translated}</entry>'
            translated_content = translated_content.replace(old_entry, new_entry, 1)

        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(translated_content)

        print(f"  [{thread_id}] ‚úÖ {file_name} saved to: {output_path}")
        return True

    def process_all_files(self):
        source_path = Path(SOURCE_DIR)
        output_path = Path(OUTPUT_DIR)

        # <<< –ó–ú–Ü–ù–ï–ù–û: –®—É–∫–∞—î–º–æ —Ñ–∞–π–ª–∏, —â–æ –ø–æ—á–∏–Ω–∞—é—Ç—å—Å—è –∑ RU_
        files = list(source_path.glob("RU_*.txt")) + list(source_path.glob("RU_*.xml"))

        if not files:
            print(f"‚ùå No files found in {SOURCE_DIR} starting with 'RU_'")
            return

        print(f"üéÆ SILKSONG UKRAINIAN TRANSLATION (from RU)")
        print(f"üìÅ Found {len(files)} files to translate")
        print(f"‚ö° Parallel processing: {MAX_PARALLEL_FILES} files")
        print(f"üì¶ Batch size: {BATCH_SIZE} entries per API call")
        print(f"ü§ñ Using model: {MODEL_NAME}")

        response = input("\n‚ö†Ô∏è  Continue with translation? (yes/no): ")
        if response.lower() not in ['yes', 'y']:
            print("Translation cancelled.")
            return

        start_time = time.time()
        with ThreadPoolExecutor(max_workers=MAX_PARALLEL_FILES) as executor:
            future_to_file = {}
            for i, file_path in enumerate(files, 1):
                relative_path = file_path.relative_to(source_path)
                # <<< –ó–ú–Ü–ù–ï–ù–û: –ó–∞–º—ñ–Ω—é—î–º–æ RU_ –Ω–∞ UA_ —É –Ω–∞–∑–≤—ñ –≤–∏—Ö—ñ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª—É
                output_filename = file_path.name.replace("RU_", "UA_", 1)
                output_file = output_path / relative_path.with_name(output_filename)

                future = executor.submit(self.process_file, str(file_path), str(output_file), i, len(files))
                future_to_file[future] = file_path.name

            for future in as_completed(future_to_file):
                try:
                    future.result()
                except Exception as exc:
                    print(f'‚ùå {future_to_file[future]} generated an exception: {exc}')

        elapsed_time = time.time() - start_time
        print(f"\n{'=' * 50}")
        print(f"‚úÖ TRANSLATION COMPLETED in {elapsed_time / 60:.1f} minutes!")
        print(f"üìä Statistics:")
        print(f"  - Translated: {self.stats['translated']} strings")
        print(f"  - From cache: {self.stats['cached']} strings")
        print(f"  - Errors: {self.stats['errors']} strings")
        print(f"  - API calls: {self.stats['api_calls']}")
        print(f"  - Tokens used: {self.stats['tokens_used']:,}")

        cost = (self.stats['tokens_used'] / 1_000_000) * 1.5  # –ü—Ä–∏–±–ª–∏–∑–Ω–∞ –≤–∞—Ä—Ç—ñ—Å—Ç—å gpt-4o
        print(f"üíµ Actual cost: ${cost:.3f}")
        print(f"üìÅ Output folder: {OUTPUT_DIR}")
        print(f"üíæ Cache saved to: translation_cache.json")

    def save_cache(self):
        with self.cache_lock:
            with open("translation_cache.json", 'w', encoding='utf-8') as f:
                json.dump(self.translation_cache, f, ensure_ascii=False, indent=2)

    def load_cache(self):
        try:
            with open("translation_cache.json", 'r', encoding='utf-8') as f:
                self.translation_cache = json.load(f)
                print(f"üìö Loaded {len(self.translation_cache)} cached translations")
        except FileNotFoundError:
            print("üìö Starting with empty translation cache")


# ... (—Ä–µ—à—Ç–∞ –∫–æ–¥—É –±–µ–∑ –∑–º—ñ–Ω) ...

def test_connection():
    print("üîç Testing OpenAI API connection...")
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": "Translate to Ukrainian: Hello"}],
            max_completion_tokens=10
        )
        result = response.choices[0].message.content
        print(f"‚úÖ Connection successful! Test response: {result}")
        return True
    except Exception as e:
        print(f"‚ùå Connection failed: {e}")
        return False


if __name__ == "__main__":
    if not OPENAI_API_KEY:
        print("‚ùå Please set your OpenAI API key in .env file!")
        exit(1)
    if not test_connection():
        exit(1)

    translator = SilksongOpenAITranslator()
    translator.process_all_files()